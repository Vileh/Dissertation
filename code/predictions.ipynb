{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import data_preprocessing as dp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary for time-aware:\n",
      "Number of images per split:\n",
      "time-aware\n",
      "train     5182\n",
      "test      1070\n",
      "val        680\n",
      "ignore     650\n",
      "Name: count, dtype: int64\n",
      "Proportions of train/val/test splits:\n",
      "time-aware\n",
      "train     0.683461\n",
      "test      0.141124\n",
      "val       0.089686\n",
      "ignore    0.085729\n",
      "Name: count, dtype: float64\n",
      "Number of individuals in train split: 357\n",
      "Number of individuals in val split: 53\n",
      "Number of individuals in test split: 75\n",
      "\n",
      "Summary for encounter:\n",
      "Number of images per split:\n",
      "encounter\n",
      "train    5552\n",
      "test     1035\n",
      "val       995\n",
      "Name: count, dtype: int64\n",
      "Proportions of train/val/test splits:\n",
      "encounter\n",
      "train    0.732261\n",
      "test     0.136508\n",
      "val      0.131232\n",
      "Name: count, dtype: float64\n",
      "Number of individuals in train split: 400\n",
      "Number of individuals in val split: 349\n",
      "Number of individuals in test split: 388\n",
      "\n",
      "Summary for random:\n",
      "Number of images per split:\n",
      "random\n",
      "train    5785\n",
      "test     1073\n",
      "val       724\n",
      "Name: count, dtype: int64\n",
      "Proportions of train/val/test splits:\n",
      "random\n",
      "train    0.762991\n",
      "test     0.141519\n",
      "val      0.095489\n",
      "Name: count, dtype: float64\n",
      "Number of individuals in train split: 400\n",
      "Number of individuals in val split: 255\n",
      "Number of individuals in test split: 307\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load split df\n",
    "df = pd.read_csv('../results/splits_trained_on.csv')\n",
    "\n",
    "# Define split columns\n",
    "split_columns = ['time-aware', 'encounter', 'random']\n",
    "\n",
    "# Iterate over each split column and print summary statistics\n",
    "for split_column in split_columns:\n",
    "    dp.print_split_summary(df, split_column)\n",
    "\n",
    "# Get label maps\n",
    "label_map, inverse_label_map = dp.get_data_label_map(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from models import load_model_and_data\n",
    "from eval import predict_ce, predict_arc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = ['crossentropy', 'arcface']\n",
    "splits = ['encounter']\n",
    "augs = ['base', 'mixed']\n",
    "\n",
    "for split in splits:\n",
    "    for loss in losses:\n",
    "        for aug in augs:\n",
    "            # Loading\n",
    "            model_name = loss + '_' + aug + '_' + split\n",
    "            result = load_model_and_data(model_name + '.pth', loss, split, label_map)\n",
    "\n",
    "            model = result['model']\n",
    "            test_loader = result['test_loader']\n",
    "            clean_train_loader = result['clean_train_loader']\n",
    "\n",
    "            # Define column name\n",
    "            pred_column_name = model_name + '_none'\n",
    "            df[pred_column_name] = 'None'\n",
    "\n",
    "            if loss == 'arcface':\n",
    "                predictions, ground_truth = predict_arc(model, test_loader, clean_train_loader)\n",
    "            elif loss == 'crossentropy':\n",
    "                predictions, ground_truth = predict_ce(model, test_loader)\n",
    "\n",
    "            df.loc[df[split]==\"test\", pred_column_name] = [inverse_label_map[split][prediction] for prediction in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = ['crossentropy']\n",
    "splits = ['encounter']\n",
    "\n",
    "for split in splits:\n",
    "    for loss in losses:\n",
    "        # Loading\n",
    "        model_name = loss + '_mixed_' + split\n",
    "        result = load_model_and_data(model_name + '.pth', loss, split, label_map)\n",
    "\n",
    "        model = result['model']\n",
    "        test_loader = result['test_loader']\n",
    "        clean_train_loader = result['clean_train_loader']\n",
    "\n",
    "        # Define column name\n",
    "        pred_column_name = loss + '_' + split + '_base_none'\n",
    "        df[pred_column_name] = 'None'\n",
    "\n",
    "        if loss == 'arcface':\n",
    "            predictions, ground_truth = predict_arc(model, test_loader, clean_train_loader)\n",
    "        elif loss == 'crossentropy':\n",
    "            predictions, ground_truth = predict_ce(model, test_loader)\n",
    "\n",
    "        df.loc[df[split]==\"test\", pred_column_name] = [inverse_label_map[split][prediction] for prediction in predictions]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Low Resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import torchvision.transforms.functional as TF\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "from PIL import Image\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resolution_distortion(images, scale_factor=0.5):\n",
    "    \"\"\"\n",
    "    Lowers the resolution of the images and then scales them back to 224x224.\n",
    "    \n",
    "    Args:\n",
    "    images (torch.Tensor): Input images of shape (B, C, H, W)\n",
    "    scale_factor (float): Factor to scale down the image resolution\n",
    "    \n",
    "    Returns:\n",
    "    torch.Tensor: Distorted images of shape (B, C, 224, 224)\n",
    "    \"\"\"\n",
    "    B, C, H, W = images.shape\n",
    "    \n",
    "    # Calculate new dimensions\n",
    "    new_H, new_W = int(H * scale_factor), int(W * scale_factor)\n",
    "    \n",
    "    # Define the transformation pipeline\n",
    "    transform = T.Compose([\n",
    "        T.Resize((new_H, new_W), antialias=True),  # Lower resolution\n",
    "        T.Resize((224, 224), antialias=True)  # Scale back to 224x224\n",
    "    ])\n",
    "    \n",
    "    # Apply the transformation\n",
    "    distorted_images = torch.stack([transform(img) for img in images])\n",
    "    \n",
    "    return distorted_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_factors = [1, 0.75, 0.5, 0.25, 0.15, 0.1]\n",
    "\n",
    "losses = ['crossentropy']\n",
    "splits = ['encounter']\n",
    "\n",
    "for split in splits:\n",
    "    for loss in losses:\n",
    "        for sf in scale_factors:\n",
    "            # Loading\n",
    "            model_name = loss + '_base_' + split\n",
    "            distortion_func=lambda x: resolution_distortion(x, scale_factor=sf)\n",
    "            result = load_model_and_data(model_name + '.pth', loss, split, label_map)\n",
    "\n",
    "            model = result['model']\n",
    "            test_loader = result['test_loader']\n",
    "            clean_train_loader = result['clean_train_loader']\n",
    "\n",
    "            # Define column name\n",
    "            pred_column_name = model_name + f'_lr(sf={sf})'\n",
    "            df[pred_column_name] = 'None'\n",
    "\n",
    "            if loss == 'arcface':\n",
    "                predictions, ground_truth = predict_arc(model, test_loader, clean_train_loader, distortion_func=distortion_func)\n",
    "            elif loss == 'crossentropy':\n",
    "                predictions, ground_truth = predict_ce(model, test_loader, distortion_func=distortion_func)\n",
    "\n",
    "            df.loc[df[split]==\"test\", pred_column_name] = [inverse_label_map[split][prediction] for prediction in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_factors = [1, 0.75, 0.5, 0.25, 0.15, 0.1]\n",
    "\n",
    "losses = ['arcface']\n",
    "splits = ['encounter']\n",
    "\n",
    "for split in splits:\n",
    "    for loss in losses:\n",
    "        for sf in scale_factors:\n",
    "            # Loading\n",
    "            model_name = loss + '_mixed_' + split\n",
    "            distortion_func=lambda x: resolution_distortion(x, scale_factor=sf)\n",
    "            result = load_model_and_data(model_name + '.pth', loss, split, label_map)\n",
    "\n",
    "            model = result['model']\n",
    "            test_loader = result['test_loader']\n",
    "            clean_train_loader = result['clean_train_loader']\n",
    "\n",
    "            # Define column name\n",
    "            pred_column_name = model_name + f'_lr(sf={sf})'\n",
    "            df[pred_column_name] = 'None'\n",
    "\n",
    "            if loss == 'arcface':\n",
    "                predictions, ground_truth = predict_arc(model, test_loader, clean_train_loader, distortion_func=distortion_func)\n",
    "            elif loss == 'crossentropy':\n",
    "                predictions, ground_truth = predict_ce(model, test_loader, distortion_func=distortion_func)\n",
    "\n",
    "            df.loc[df[split]==\"test\", pred_column_name] = [inverse_label_map[split][prediction] for prediction in predictions]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motion Blur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Page where I got motion blur from: https://www.kaggle.com/code/limitz/random-motion-blur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def random_motion(steps=16, initial_vector=None, alpha=0.2):\n",
    "    if initial_vector is None:\n",
    "        initial_vector = torch.randn(1, dtype=torch.cfloat)\n",
    "    \n",
    "    # Generate the random motion path\n",
    "    motion = [torch.zeros_like(initial_vector)]\n",
    "    for _ in range(steps):\n",
    "        change = torch.randn(initial_vector.shape[0], dtype=torch.cfloat)\n",
    "        initial_vector = initial_vector + change * alpha\n",
    "        initial_vector /= initial_vector.abs().add(1e-8)\n",
    "        motion.append(motion[-1] + initial_vector)\n",
    "    \n",
    "    motion = torch.stack(motion, -1)\n",
    "    \n",
    "    # Find bounding box\n",
    "    real_min, _ = motion.real.min(dim=-1, keepdim=True)\n",
    "    real_max, _ = motion.real.max(dim=-1, keepdim=True)\n",
    "    imag_min, _ = motion.imag.min(dim=-1, keepdim=True)\n",
    "    imag_max, _ = motion.imag.max(dim=-1, keepdim=True)\n",
    "\n",
    "    # Scale motion to fit exactly in steps x steps\n",
    "    real_scale = (steps - 1) / (real_max - real_min)\n",
    "    imag_scale = (steps - 1) / (imag_max - imag_min)\n",
    "    scale = torch.min(real_scale, imag_scale)\n",
    "    \n",
    "    real_shift = (steps - (real_max - real_min) * scale) / 2 - real_min * scale\n",
    "    imag_shift = (steps - (imag_max - imag_min) * scale) / 2 - imag_min * scale\n",
    "    \n",
    "    scaled_motion = motion * scale + (real_shift + 1j * imag_shift)\n",
    "    \n",
    "    # Create kernel\n",
    "    kernel = torch.zeros(initial_vector.shape[0], 1, steps, steps)\n",
    "    \n",
    "    # Fill kernel\n",
    "    for s in range(steps + 1):\n",
    "        v = scaled_motion[:, s]\n",
    "        x = torch.clamp(v.real, 0, steps - 1)\n",
    "        y = torch.clamp(v.imag, 0, steps - 1)\n",
    "        \n",
    "        ix = x.long()\n",
    "        iy = y.long()\n",
    "        \n",
    "        vx = x - ix.float()\n",
    "        vy = y - iy.float()\n",
    "        \n",
    "        for i in range(initial_vector.shape[0]):\n",
    "            kernel[i, 0, iy[i], ix[i]] += (1-vx[i]) * (1-vy[i]) / steps\n",
    "            if ix[i] + 1 < steps:\n",
    "                kernel[i, 0, iy[i], ix[i]+1] += vx[i] * (1-vy[i]) / steps\n",
    "            if iy[i] + 1 < steps:\n",
    "                kernel[i, 0, iy[i]+1, ix[i]] += (1-vx[i]) * vy[i] / steps\n",
    "            if ix[i] + 1 < steps and iy[i] + 1 < steps:\n",
    "                kernel[i, 0, iy[i]+1, ix[i]+1] += vx[i] * vy[i] / steps\n",
    "\n",
    "    # Normalize the kernel\n",
    "    kernel /= kernel.sum(dim=(-1, -2), keepdim=True)\n",
    "    \n",
    "    return kernel\n",
    "\n",
    "class RandomMotionBlur(nn.Module):\n",
    "    \"\"\"\n",
    "    Apply random motion blur to input tensors.\n",
    "    \"\"\"\n",
    "    def __init__(self, steps=17, alpha=0.2):\n",
    "        \"\"\"\n",
    "        Initialize the RandomMotionBlur module.\n",
    "        \n",
    "        Args:\n",
    "        - steps (int): Number of steps in the motion path\n",
    "        - alpha (float): Controls the randomness of the motion path\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.steps = steps\n",
    "        self.alpha = alpha\n",
    "       \n",
    "    def forward(self, x, return_kernel=False):\n",
    "        \"\"\"\n",
    "        Apply random motion blur to the input tensor.\n",
    "        \n",
    "        Args:\n",
    "        - x (torch.Tensor): Input tensor to be blurred\n",
    "        - return_kernel (bool): If True, return both blurred tensor and kernel\n",
    "        \n",
    "        Returns:\n",
    "        - y (torch.Tensor): Blurred tensor\n",
    "        - m (torch.Tensor, optional): Blur kernel, if return_kernel is True\n",
    "        \"\"\"\n",
    "        # Add an extra dimension in x\n",
    "        if x.dim() == 3:\n",
    "            x = x.unsqueeze(0)\n",
    "\n",
    "        # Generate a random initial vector\n",
    "        vector = torch.randn(x.shape[0], dtype=torch.cfloat) / 3\n",
    "        vector.real /= 2\n",
    "        \n",
    "        # Create the motion blur kernel\n",
    "        m = random_motion(self.steps, vector, alpha=self.alpha)\n",
    "        \n",
    "        # Pad the input tensor for convolution\n",
    "        xpad = [m.shape[-1]//2+1] * 2 + [m.shape[-2]//2+1] * 2\n",
    "        x = F.pad(x, xpad)\n",
    "        \n",
    "        # Pad the kernel to match input size\n",
    "        mpad = [0, x.shape[-1]-m.shape[-1], 0, x.shape[-2]-m.shape[-2]]\n",
    "        mp = F.pad(m, mpad)\n",
    "        \n",
    "        # Apply blur in the frequency domain\n",
    "        fx = torch.fft.fft2(x)  # FFT of input\n",
    "        fm = torch.fft.fft2(mp)  # FFT of kernel\n",
    "        fy = fx * fm  # Multiplication in frequency domain\n",
    "        y = torch.fft.ifft2(fy).real  # Inverse FFT to get blurred result\n",
    "        \n",
    "        # Crop the result to original size\n",
    "        y = y[...,xpad[2]:-xpad[3], xpad[0]:-xpad[1]]\n",
    "        \n",
    "        return y if not return_kernel else (y, m)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_sizes = [4, 8, 12, 16, 20, 24]\n",
    "\n",
    "for split in ['encounter']:\n",
    "    for loss in ['crossentropy']:\n",
    "        for ss in step_sizes:\n",
    "            # Loading\n",
    "            model_name = loss + '_base_' + split\n",
    "            blur = RandomMotionBlur(steps=ss)\n",
    "            distortion_func=lambda x: blur(x)\n",
    "            result = load_model_and_data(model_name + '.pth', loss, split, label_map)\n",
    "\n",
    "            model = result['model']  \n",
    "            test_loader = result['test_loader']\n",
    "            clean_train_loader = result['clean_train_loader']\n",
    "\n",
    "            # Define column name\n",
    "            pred_column_name = model_name + f'_mb(ss={ss})'\n",
    "            df[pred_column_name] = 'None'\n",
    "\n",
    "            if loss == 'arcface':\n",
    "                predictions, ground_truth = predict_arc(model, test_loader, clean_train_loader, distortion_func=distortion_func)\n",
    "            elif loss == 'crossentropy':\n",
    "                predictions, ground_truth = predict_ce(model, test_loader, distortion_func=distortion_func)\n",
    "\n",
    "            df.loc[df[split]==\"test\", pred_column_name] = [inverse_label_map[split][prediction] for prediction in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_sizes = [4, 8, 12, 16, 20, 24]\n",
    "\n",
    "for split in ['encounter']:\n",
    "    for loss in ['arcface']:\n",
    "        for ss in step_sizes:\n",
    "            # Loading\n",
    "            model_name = loss + '_mixed_' + split\n",
    "            blur = RandomMotionBlur(steps=ss)\n",
    "            distortion_func=lambda x: blur(x)\n",
    "            result = load_model_and_data(model_name + '.pth', loss, split, label_map)\n",
    "\n",
    "            model = result['model']  \n",
    "            test_loader = result['test_loader']\n",
    "            clean_train_loader = result['clean_train_loader']\n",
    "\n",
    "            # Define column name\n",
    "            pred_column_name = model_name + f'_mb(ss={ss})'\n",
    "            df[pred_column_name] = 'None'\n",
    "\n",
    "            if loss == 'arcface':\n",
    "                predictions, ground_truth = predict_arc(model, test_loader, clean_train_loader, distortion_func=distortion_func)\n",
    "            elif loss == 'crossentropy':\n",
    "                predictions, ground_truth = predict_ce(model, test_loader, distortion_func=distortion_func)\n",
    "\n",
    "            df.loc[df[split]==\"test\", pred_column_name] = [inverse_label_map[split][prediction] for prediction in predictions]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Blur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "def gaussian_blur(images, kernel_size=5, sigma=2.0):\n",
    "    return TF.gaussian_blur(images, kernel_size, sigma) if sigma > 0 else images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_sizes = [3, 5, 7, 9, 11, 13, 15, 17]\n",
    "sigma_values = [1.0, 2.0, 3.0, 5.0, 7.0, 10.0]\n",
    "\n",
    "models = ['base', 'mixed']\n",
    "\n",
    "for split in ['encounter']:\n",
    "    for loss in ['arcface', 'crossentropy']:\n",
    "        for model_type in models:\n",
    "            print(model_type)\n",
    "            for kernel_size in kernel_sizes:\n",
    "                for sigma in sigma_values:\n",
    "                    # Loading\n",
    "                    model_name = loss + '_' + model_type + '_' + split\n",
    "                    distortion_func=lambda x: gaussian_blur(x, kernel_size=kernel_size, sigma=sigma)\n",
    "                    result = load_model_and_data(model_name + '.pth', loss, split, label_map)\n",
    "\n",
    "                    model = result['model']\n",
    "                    test_loader = result['test_loader']\n",
    "                    clean_train_loader = result['clean_train_loader']\n",
    "\n",
    "                    # Define column name\n",
    "                    pred_column_name = model_name + f'_gb(ks={kernel_size}, sigma={sigma})'\n",
    "                    df[pred_column_name] = 'None'\n",
    "\n",
    "                    if loss == 'arcface':\n",
    "                        predictions, ground_truth = predict_arc(model, test_loader, clean_train_loader, distortion_func=distortion_func)\n",
    "                    elif loss == 'crossentropy':\n",
    "                        predictions, ground_truth = predict_ce(model, test_loader, distortion_func=distortion_func)\n",
    "\n",
    "                    df.loc[df[split]==\"test\", pred_column_name] = [inverse_label_map[split][prediction] for prediction in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../results/predictions/all_predictions.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
